{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personality Text Classification.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee3206/Cylons_Personalities/blob/master/Personality_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ic4_occAAiAT"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "ioaprt5q5US7",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "yCl0eTNH5RS3",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ItXfxkxvosLH"
      },
      "source": [
        "# Text classification with TensorFlow Hub: Movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Eg62Pmz3o83v"
      },
      "source": [
        "\n",
        "This notebook classifies movie reviews as *positive* or *negative* using the text of the review. This is an example of *binary*—or two-class—classification, an important and widely applicable kind of machine learning problem.\n",
        "\n",
        "The tutorial demonstrates the basic application of transfer learning with TensorFlow Hub and Keras.\n",
        "\n",
        "We'll use the [IMDB dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/). These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are *balanced*, meaning they contain an equal number of positive and negative reviews. \n",
        "\n",
        "This notebook uses [tf.keras](https://www.tensorflow.org/guide/keras), a high-level API to build and train models in TensorFlow, and [TensorFlow Hub](https://www.tensorflow.org/hub), a library and platform for transfer learning. For a more advanced text classification tutorial using `tf.keras`, see the [MLCC Text Classification Guide](https://developers.google.com/machine-learning/guides/text-classification/).\n",
        "\n",
        "PULL INFO FROM TEXT IMPORT TUTORIAL [HERE](https://www.tensorflow.org/tutorials/load_data/text)!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ew7HTbPpCJH",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import functools\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "!pip install -q tensorflow-hub\n",
        "!pip install -q tfds-nightly\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iAsKG535pHep"
      },
      "source": [
        "## Check that you have the file/path\n",
        "\n",
        "Files are hosted on a google drive, should play nicely without any issues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VO6_gv2UN7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "TRAIN_DATA_URL = \"https://drive.google.com/uc?export=view&id=1ACCsnWRl-cO_rQb3iwkDmeaHavAiHfUN\"\n",
        "train_file_path = tf.keras.utils.get_file(\"myPersonality_Training.csv\", TRAIN_DATA_URL)\n",
        "\n",
        "TEST_DATA_URL = \"https://drive.google.com/uc?export=view&id=1k1zcfi7O3cwt3S4-iJm_Q1a0ZOGj-fsF\"\n",
        "test_file_path =  tf.keras.utils.get_file(\"myPersonality_Test.csv\", TEST_DATA_URL)\n",
        "\n",
        "VAL_DATA_URL = \"https://drive.google.com/uc?export=view&id=1xG4R4pgIDCpiCCjxgP9YbH_SSm-XufwF\"\n",
        "val_file_path = tf.keras.utils.get_file(\"myPersonality_Validation.csv\", VAL_DATA_URL)\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "#Replaces the !head fuction of bash\n",
        "with open(train_file_path) as f:\n",
        "    for _ in range(10):\n",
        "        print(f.readline())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJQB2i3qgwIr",
        "colab_type": "text"
      },
      "source": [
        "This uses the sEXT column (or Extraversion) as the column of interest. I'm not sure how to get this to work with multiple attributes (it might be more of a regression but I want to talk to someone more knowledgeable to see)\n",
        "\n",
        "It builds a dataset object from the csv thats been loaded in, and then shows a small batch in the output to take a look at the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHPcuKdoUN70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = \"sEXT\"\n",
        "\n",
        "def get_dataset(file_path, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file_path,\n",
        "      batch_size=10, # Artificially small to make examples easier to show.\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value=\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True, \n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "raw_train_data = get_dataset(train_file_path)\n",
        "#raw_test_data = get_dataset(test_file_path)\n",
        "\n",
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))\n",
        "    \n",
        "\n",
        "show_batch(raw_train_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3vhUqffUN73",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Not sure how to create multi-variable processing, so limiting to EXTRAVERSION while I play with it more\n",
        "SELECT_COLUMNS = [\"STATUS\",\"sEXT\"]\n",
        "\n",
        "sEXT_dataset = get_dataset(train_file_path, select_columns = SELECT_COLUMNS)\n",
        "test_data = get_dataset(test_file_path, select_columns = SELECT_COLUMNS)\n",
        "validation_data = get_dataset(val_file_path, select_columns = SELECT_COLUMNS)\n",
        "\n",
        "show_batch(sEXT_dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK5B_KOXUN77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separate posts and labels\n",
        "#Turns out .pop with ('STATUS') as the key seems to work (no errors and looks to be loading the data), but dies at a later point\n",
        "train_examples_batch, train_labels_batch = next(iter(sEXT_dataset))\n",
        "train_examples_batch\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN5AdSyUUN79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLC02j2g-llC"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "The neural network is created by stacking layers—this requires three main architectural decisions:\n",
        "\n",
        "* How to represent the text?\n",
        "* How many layers to use in the model?\n",
        "* How many *hidden units* to use for each layer?\n",
        "\n",
        "In this example, the input data consists of sentences. The labels to predict are either 0 or 1.\n",
        "\n",
        "One way to represent the text is to convert sentences into embeddings vectors. We can use a pre-trained text embedding as the first layer, which will have three advantages:\n",
        "\n",
        "*   we don't have to worry about text preprocessing,\n",
        "*   we can benefit from transfer learning,\n",
        "*   the embedding has a fixed size, so it's simpler to process.\n",
        "\n",
        "For this example we will use a **pre-trained text embedding model** from [TensorFlow Hub](https://www.tensorflow.org/hub) called [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1).\n",
        "\n",
        "There are three other pre-trained models to test for the sake of this tutorial:\n",
        "\n",
        "* [google/tf2-preview/gnews-swivel-20dim-with-oov/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim-with-oov/1) - same as [google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1), but with 2.5% vocabulary converted to OOV buckets. This can help if vocabulary of the task and vocabulary of the model don't fully overlap.\n",
        "* [google/tf2-preview/nnlm-en-dim50/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1) - A much larger model with ~1M vocabulary size and 50 dimensions.\n",
        "* [google/tf2-preview/nnlm-en-dim128/1](https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1) - Even larger model with ~1M vocabulary size and 128 dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "In2nDpTLkgKa"
      },
      "source": [
        "Let's first create a Keras layer that uses a TensorFlow Hub model to embed the sentences, and try it out on a couple of input examples. Note that no matter the length of the input text, the output shape of the embeddings is: `(num_examples, embedding_dimension)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NUbzVeYkgcO",
        "colab": {}
      },
      "source": [
        "#embedding = \"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1\"\n",
        "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
        "hub_layer = hub.KerasLayer(embedding, input_shape=[], dtype=tf.string, trainable=True)\n",
        "#hub_layer([\"A long sentence.\", \"single-word\",\"http://example.com\"])\n",
        "#So this returns a tf.Tensor where all of the text info has been converted into a bunch of vectors.\n",
        "pop_hub = train_examples_batch.pop('STATUS')\n",
        "hub_layer(pop_hub)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dfSbV6igl1EH"
      },
      "source": [
        "Let's now build the full model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xpKOoWgu-llD",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "#THE MODEL SEEMS TO LOAD DOESNT IT?!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6PbKQ6mucuKL"
      },
      "source": [
        "The layers are stacked sequentially to build the classifier:\n",
        "\n",
        "1. The first layer is a TensorFlow Hub layer. This layer uses a pre-trained Saved Model to map a sentence into its embedding vector. The pre-trained text embedding model that we are using ([google/tf2-preview/gnews-swivel-20dim/1](https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1)) splits the sentence into tokens, embeds each token and then combines the embedding. The resulting dimensions are: `(num_examples, embedding_dimension)`.\n",
        "2. This fixed-length output vector is piped through a fully-connected (`Dense`) layer with 16 hidden units.\n",
        "3. The last layer is densely connected with a single output node. Using the `sigmoid` activation function, this value is a float between 0 and 1, representing a probability, or confidence level.\n",
        "\n",
        "Let's compile the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L4EqVWg4-llM"
      },
      "source": [
        "### Loss function and optimizer\n",
        "\n",
        "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
        "\n",
        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
        "\n",
        "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
        "\n",
        "Now, configure the model to use an optimizer and a loss function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mr0GP-cQ-llN",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#This seems to work as well"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "35jv_fzP-llU"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Train the model for 20 epochs in mini-batches of 512 samples. This is 20 iterations over all samples in the `x_train` and `y_train` tensors. While training, monitor the model's loss and accuracy on the 10,000 samples from the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tXSGrjWZ-llW",
        "colab": {}
      },
      "source": [
        "history = model.fit(sEXT_dataset.shuffle(4860).batch(512),\n",
        "                    epochs=20,\n",
        "                    validation_data=validation_data.batch(5),\n",
        "                    verbose=1)\n",
        "\n",
        "#Then it craps out here by saying that there isn't any data provided and everything needs a key"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9EEGuDVuzb5r"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "And let's see how the model performs. Two values will be returned. Loss (a number which represents our error, lower values are better), and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zOMKywn4zReN",
        "colab": {}
      },
      "source": [
        "results = model.evaluate(test_data.batch(100), verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z1iEXVTR0Z2t"
      },
      "source": [
        "This fairly naive approach achieves an accuracy of about 87%. With more advanced approaches, the model should get closer to 95%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5KggXVeL-llZ"
      },
      "source": [
        "## Further reading\n",
        "\n",
        "For a more general way to work with string inputs and for a more detailed analysis of the progress of accuracy and loss during training, take a look [here](https://www.tensorflow.org/tutorials/keras/basic_text_classification)."
      ]
    }
  ]
}